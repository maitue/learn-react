{"ast":null,"code":"var __extends = this && this.__extends || function () {\n  var _extendStatics = function extendStatics(d, b) {\n    _extendStatics = Object.setPrototypeOf || {\n      __proto__: []\n    } instanceof Array && function (d, b) {\n      d.__proto__ = b;\n    } || function (d, b) {\n      for (var p in b) {\n        if (b.hasOwnProperty(p)) d[p] = b[p];\n      }\n    };\n\n    return _extendStatics(d, b);\n  };\n\n  return function (d, b) {\n    _extendStatics(d, b);\n\n    function __() {\n      this.constructor = d;\n    }\n\n    d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n  };\n}();\n\nvar __assign = this && this.__assign || function () {\n  __assign = Object.assign || function (t) {\n    for (var s, i = 1, n = arguments.length; i < n; i++) {\n      s = arguments[i];\n\n      for (var p in s) {\n        if (Object.prototype.hasOwnProperty.call(s, p)) t[p] = s[p];\n      }\n    }\n\n    return t;\n  };\n\n  return __assign.apply(this, arguments);\n};\n\nvar __awaiter = this && this.__awaiter || function (thisArg, _arguments, P, generator) {\n  function adopt(value) {\n    return value instanceof P ? value : new P(function (resolve) {\n      resolve(value);\n    });\n  }\n\n  return new (P || (P = Promise))(function (resolve, reject) {\n    function fulfilled(value) {\n      try {\n        step(generator.next(value));\n      } catch (e) {\n        reject(e);\n      }\n    }\n\n    function rejected(value) {\n      try {\n        step(generator[\"throw\"](value));\n      } catch (e) {\n        reject(e);\n      }\n    }\n\n    function step(result) {\n      result.done ? resolve(result.value) : adopt(result.value).then(fulfilled, rejected);\n    }\n\n    step((generator = generator.apply(thisArg, _arguments || [])).next());\n  });\n};\n\nvar __generator = this && this.__generator || function (thisArg, body) {\n  var _ = {\n    label: 0,\n    sent: function sent() {\n      if (t[0] & 1) throw t[1];\n      return t[1];\n    },\n    trys: [],\n    ops: []\n  },\n      f,\n      y,\n      t,\n      g;\n  return g = {\n    next: verb(0),\n    \"throw\": verb(1),\n    \"return\": verb(2)\n  }, typeof Symbol === \"function\" && (g[Symbol.iterator] = function () {\n    return this;\n  }), g;\n\n  function verb(n) {\n    return function (v) {\n      return step([n, v]);\n    };\n  }\n\n  function step(op) {\n    if (f) throw new TypeError(\"Generator is already executing.\");\n\n    while (_) {\n      try {\n        if (f = 1, y && (t = op[0] & 2 ? y[\"return\"] : op[0] ? y[\"throw\"] || ((t = y[\"return\"]) && t.call(y), 0) : y.next) && !(t = t.call(y, op[1])).done) return t;\n        if (y = 0, t) op = [op[0] & 2, t.value];\n\n        switch (op[0]) {\n          case 0:\n          case 1:\n            t = op;\n            break;\n\n          case 4:\n            _.label++;\n            return {\n              value: op[1],\n              done: false\n            };\n\n          case 5:\n            _.label++;\n            y = op[1];\n            op = [0];\n            continue;\n\n          case 7:\n            op = _.ops.pop();\n\n            _.trys.pop();\n\n            continue;\n\n          default:\n            if (!(t = _.trys, t = t.length > 0 && t[t.length - 1]) && (op[0] === 6 || op[0] === 2)) {\n              _ = 0;\n              continue;\n            }\n\n            if (op[0] === 3 && (!t || op[1] > t[0] && op[1] < t[3])) {\n              _.label = op[1];\n              break;\n            }\n\n            if (op[0] === 6 && _.label < t[1]) {\n              _.label = t[1];\n              t = op;\n              break;\n            }\n\n            if (t && _.label < t[2]) {\n              _.label = t[2];\n\n              _.ops.push(op);\n\n              break;\n            }\n\n            if (t[2]) _.ops.pop();\n\n            _.trys.pop();\n\n            continue;\n        }\n\n        op = body.call(thisArg, _);\n      } catch (e) {\n        op = [6, e];\n        y = 0;\n      } finally {\n        f = t = 0;\n      }\n    }\n\n    if (op[0] & 5) throw op[1];\n    return {\n      value: op[0] ? op[1] : void 0,\n      done: true\n    };\n  }\n};\n\nimport { Credentials, getAmplifyUserAgent } from '@aws-amplify/core';\nimport Storage from '@aws-amplify/storage';\nimport { AbstractIdentifyPredictionsProvider } from '../types/Providers';\nimport { RekognitionClient, SearchFacesByImageCommand, DetectTextCommand, DetectLabelsCommand, DetectFacesCommand, DetectModerationLabelsCommand, RecognizeCelebritiesCommand } from '@aws-sdk/client-rekognition';\nimport { isStorageSource, isFileSource, isBytesSource, isIdentifyCelebrities, isIdentifyFromCollection } from '../types';\nimport { TextractClient, DetectDocumentTextCommand, AnalyzeDocumentCommand } from '@aws-sdk/client-textract';\nimport { makeCamelCase, makeCamelCaseArray, blobToArrayBuffer } from './Utils';\nimport { categorizeRekognitionBlocks, categorizeTextractBlocks } from './IdentifyTextUtils';\n\nvar AmazonAIIdentifyPredictionsProvider =\n/** @class */\nfunction (_super) {\n  __extends(AmazonAIIdentifyPredictionsProvider, _super);\n\n  function AmazonAIIdentifyPredictionsProvider() {\n    return _super.call(this) || this;\n  }\n\n  AmazonAIIdentifyPredictionsProvider.prototype.getProviderName = function () {\n    return 'AmazonAIIdentifyPredictionsProvider';\n  };\n  /**\n   * Verify user input source and converts it into source object readable by Rekognition and Textract.\n   * Note that Rekognition and Textract use the same source interface, so we need not worry about types.\n   * @param {IdentifySource} source - User input source that directs to the object user wants\n   * to identify (storage, file, or bytes).\n   * @return {Promise<Image>} - Promise resolving to the converted source object.\n   */\n\n\n  AmazonAIIdentifyPredictionsProvider.prototype.configureSource = function (source) {\n    return new Promise(function (res, rej) {\n      if (isStorageSource(source)) {\n        var storageConfig = {\n          level: source.level,\n          identityId: source.identityId\n        };\n        Storage.get(source.key, storageConfig).then(function (url) {\n          var parser = /https:\\/\\/([a-zA-Z0-9%-_.]+)\\.s3\\.[A-Za-z0-9%-._~]+\\/([a-zA-Z0-9%-._~/]+)\\?/;\n          var parsedURL = url.match(parser);\n          if (parsedURL.length < 3) rej('Invalid S3 key was given.');\n          res({\n            S3Object: {\n              Bucket: parsedURL[1],\n              Name: decodeURIComponent(parsedURL[2])\n            }\n          });\n        }).catch(function (err) {\n          return rej(err);\n        });\n      } else if (isFileSource(source)) {\n        blobToArrayBuffer(source.file).then(function (buffer) {\n          res({\n            Bytes: new Uint8Array(buffer)\n          });\n        }).catch(function (err) {\n          return rej(err);\n        });\n      } else if (isBytesSource(source)) {\n        var bytes = source.bytes;\n\n        if (bytes instanceof Blob) {\n          blobToArrayBuffer(bytes).then(function (buffer) {\n            res({\n              Bytes: new Uint8Array(buffer)\n            });\n          }).catch(function (err) {\n            return rej(err);\n          });\n        }\n\n        if (bytes instanceof ArrayBuffer || bytes instanceof Buffer) {\n          res({\n            Bytes: new Uint8Array(bytes)\n          });\n        } // everything else can be directly passed to Rekognition / Textract.\n\n\n        res({\n          Bytes: bytes\n        });\n      } else {\n        rej('Input source is not configured correctly.');\n      }\n    });\n  };\n  /**\n   * Recognize text from real-world images and documents (plain text, forms and tables). Detects text in the input\n   * image and converts it into machine-readable text.\n   * @param {IdentifySource} source - Object containing the source image and feature types to analyze.\n   * @return {Promise<IdentifyTextOutput>} - Promise resolving to object containing identified texts.\n   */\n\n\n  AmazonAIIdentifyPredictionsProvider.prototype.identifyText = function (input) {\n    return __awaiter(this, void 0, void 0, function () {\n      var credentials, _a, _b, _c, region, _d, _e, configFormat, inputDocument, err_1, format, featureTypes, textractParam, rekognitionParam, detectTextCommand, rekognitionData, rekognitionResponse, detectDocumentTextCommand, Blocks, err_2, param, analyzeDocumentCommand, Blocks, err_3;\n\n      return __generator(this, function (_f) {\n        switch (_f.label) {\n          case 0:\n            return [4\n            /*yield*/\n            , Credentials.get()];\n\n          case 1:\n            credentials = _f.sent();\n            if (!credentials) return [2\n            /*return*/\n            , Promise.reject('No credentials')];\n            _a = this._config.identifyText, _b = _a === void 0 ? {} : _a, _c = _b.region, region = _c === void 0 ? '' : _c, _d = _b.defaults, _e = (_d === void 0 ? {} : _d).format, configFormat = _e === void 0 ? 'PLAIN' : _e;\n            this.rekognitionClient = new RekognitionClient({\n              region: region,\n              credentials: credentials,\n              customUserAgent: getAmplifyUserAgent()\n            });\n            this.textractClient = new TextractClient({\n              region: region,\n              credentials: credentials,\n              customUserAgent: getAmplifyUserAgent()\n            });\n            _f.label = 2;\n\n          case 2:\n            _f.trys.push([2, 4,, 5]);\n\n            return [4\n            /*yield*/\n            , this.configureSource(input.text.source)];\n\n          case 3:\n            inputDocument = _f.sent();\n            return [3\n            /*break*/\n            , 5];\n\n          case 4:\n            err_1 = _f.sent();\n            return [2\n            /*return*/\n            , Promise.reject(err_1)];\n\n          case 5:\n            format = input.text.format || configFormat;\n            featureTypes = [];\n            if (format === 'FORM' || format === 'ALL') featureTypes.push('FORMS');\n            if (format === 'TABLE' || format === 'ALL') featureTypes.push('TABLES');\n            if (!(featureTypes.length === 0)) return [3\n            /*break*/\n            , 11];\n            textractParam = {\n              Document: inputDocument\n            };\n            rekognitionParam = {\n              Image: inputDocument\n            };\n            _f.label = 6;\n\n          case 6:\n            _f.trys.push([6, 9,, 10]);\n\n            detectTextCommand = new DetectTextCommand(rekognitionParam);\n            return [4\n            /*yield*/\n            , this.rekognitionClient.send(detectTextCommand)];\n\n          case 7:\n            rekognitionData = _f.sent();\n            rekognitionResponse = categorizeRekognitionBlocks(rekognitionData.TextDetections);\n\n            if (rekognitionResponse.text.words.length < 50) {\n              // did not hit the word limit, return the data\n              return [2\n              /*return*/\n              , rekognitionResponse];\n            }\n\n            detectDocumentTextCommand = new DetectDocumentTextCommand(textractParam);\n            return [4\n            /*yield*/\n            , this.textractClient.send(detectDocumentTextCommand)];\n\n          case 8:\n            Blocks = _f.sent().Blocks;\n\n            if (rekognitionData.TextDetections.length > Blocks.length) {\n              return [2\n              /*return*/\n              , rekognitionResponse];\n            }\n\n            return [2\n            /*return*/\n            , categorizeTextractBlocks(Blocks)];\n\n          case 9:\n            err_2 = _f.sent();\n            Promise.reject(err_2);\n            return [3\n            /*break*/\n            , 10];\n\n          case 10:\n            return [3\n            /*break*/\n            , 15];\n\n          case 11:\n            param = {\n              Document: inputDocument,\n              FeatureTypes: featureTypes\n            };\n            _f.label = 12;\n\n          case 12:\n            _f.trys.push([12, 14,, 15]);\n\n            analyzeDocumentCommand = new AnalyzeDocumentCommand(param);\n            return [4\n            /*yield*/\n            , this.textractClient.send(analyzeDocumentCommand)];\n\n          case 13:\n            Blocks = _f.sent().Blocks;\n            return [2\n            /*return*/\n            , categorizeTextractBlocks(Blocks)];\n\n          case 14:\n            err_3 = _f.sent();\n            return [2\n            /*return*/\n            , Promise.reject(err_3)];\n\n          case 15:\n            return [2\n            /*return*/\n            ];\n        }\n      });\n    });\n  };\n  /**\n   * Identify instances of real world entities from an image and if it contains unsafe content.\n   * @param {IdentifyLabelsInput} input - Object containing the source image and entity type to identify.\n   * @return {Promise<IdentifyLabelsOutput>} - Promise resolving to an array of identified entities.\n   */\n\n\n  AmazonAIIdentifyPredictionsProvider.prototype.identifyLabels = function (input) {\n    return __awaiter(this, void 0, void 0, function () {\n      var credentials, _a, _b, _c, region, _d, _e, type, inputImage_1, param, servicePromises, entityType, err_4;\n\n      return __generator(this, function (_f) {\n        switch (_f.label) {\n          case 0:\n            _f.trys.push([0, 3,, 4]);\n\n            return [4\n            /*yield*/\n            , Credentials.get()];\n\n          case 1:\n            credentials = _f.sent();\n            if (!credentials) return [2\n            /*return*/\n            , Promise.reject('No credentials')];\n            _a = this._config.identifyLabels, _b = _a === void 0 ? {} : _a, _c = _b.region, region = _c === void 0 ? '' : _c, _d = _b.defaults, _e = (_d === void 0 ? {} : _d).type, type = _e === void 0 ? 'LABELS' : _e;\n            this.rekognitionClient = new RekognitionClient({\n              region: region,\n              credentials: credentials,\n              customUserAgent: getAmplifyUserAgent()\n            });\n            return [4\n            /*yield*/\n            , this.configureSource(input.labels.source).then(function (data) {\n              inputImage_1 = data;\n            }).catch(function (err) {\n              return Promise.reject(err);\n            })];\n\n          case 2:\n            _f.sent();\n\n            param = {\n              Image: inputImage_1\n            };\n            servicePromises = [];\n            entityType = input.labels.type || type;\n\n            if (entityType === 'LABELS' || entityType === 'ALL') {\n              servicePromises.push(this.detectLabels(param));\n            }\n\n            if (entityType === 'UNSAFE' || entityType === 'ALL') {\n              servicePromises.push(this.detectModerationLabels(param));\n            }\n\n            return [2\n            /*return*/\n            , Promise.all(servicePromises).then(function (data) {\n              var identifyResult = {}; // concatenate resolved promises to a single object\n\n              data.forEach(function (val) {\n                identifyResult = __assign(__assign({}, identifyResult), val);\n              });\n              return identifyResult;\n            }).catch(function (err) {\n              return Promise.reject(err);\n            })];\n\n          case 3:\n            err_4 = _f.sent();\n            return [2\n            /*return*/\n            , Promise.reject(err_4)];\n\n          case 4:\n            return [2\n            /*return*/\n            ];\n        }\n      });\n    });\n  };\n  /**\n   * Calls Rekognition.detectLabels and organizes the returned data.\n   * @param {DetectLabelsInput} param - parameter to be passed onto Rekognition\n   * @return {Promise<IdentifyLabelsOutput>} - Promise resolving to organized detectLabels response.\n   */\n\n\n  AmazonAIIdentifyPredictionsProvider.prototype.detectLabels = function (param) {\n    return __awaiter(this, void 0, void 0, function () {\n      var detectLabelsCommand, data, detectLabelData, err_5;\n      return __generator(this, function (_a) {\n        switch (_a.label) {\n          case 0:\n            _a.trys.push([0, 2,, 3]);\n\n            detectLabelsCommand = new DetectLabelsCommand(param);\n            return [4\n            /*yield*/\n            , this.rekognitionClient.send(detectLabelsCommand)];\n\n          case 1:\n            data = _a.sent();\n            if (!data.Labels) return [2\n            /*return*/\n            , {\n              labels: null\n            }]; // no image was detected\n\n            detectLabelData = data.Labels.map(function (val) {\n              var boxes = val.Instances ? val.Instances.map(function (val) {\n                return makeCamelCase(val.BoundingBox);\n              }) : undefined;\n              return {\n                name: val.Name,\n                boundingBoxes: boxes,\n                metadata: {\n                  confidence: val.Confidence,\n                  parents: makeCamelCaseArray(val.Parents)\n                }\n              };\n            });\n            return [2\n            /*return*/\n            , {\n              labels: detectLabelData\n            }];\n\n          case 2:\n            err_5 = _a.sent();\n            return [2\n            /*return*/\n            , Promise.reject(err_5)];\n\n          case 3:\n            return [2\n            /*return*/\n            ];\n        }\n      });\n    });\n  };\n  /**\n   * Calls Rekognition.detectModerationLabels and organizes the returned data.\n   * @param {Rekognition.DetectLabelsRequest} param - Parameter to be passed onto Rekognition\n   * @return {Promise<IdentifyLabelsOutput>} - Promise resolving to organized detectModerationLabels response.\n   */\n\n\n  AmazonAIIdentifyPredictionsProvider.prototype.detectModerationLabels = function (param) {\n    return __awaiter(this, void 0, void 0, function () {\n      var detectModerationLabelsCommand, data, err_6;\n      return __generator(this, function (_a) {\n        switch (_a.label) {\n          case 0:\n            _a.trys.push([0, 2,, 3]);\n\n            detectModerationLabelsCommand = new DetectModerationLabelsCommand(param);\n            return [4\n            /*yield*/\n            , this.rekognitionClient.send(detectModerationLabelsCommand)];\n\n          case 1:\n            data = _a.sent();\n\n            if (data.ModerationLabels.length !== 0) {\n              return [2\n              /*return*/\n              , {\n                unsafe: 'YES'\n              }];\n            } else {\n              return [2\n              /*return*/\n              , {\n                unsafe: 'NO'\n              }];\n            }\n\n            return [3\n            /*break*/\n            , 3];\n\n          case 2:\n            err_6 = _a.sent();\n            return [2\n            /*return*/\n            , Promise.reject(err_6)];\n\n          case 3:\n            return [2\n            /*return*/\n            ];\n        }\n      });\n    });\n  };\n  /**\n   * Identify faces within an image that is provided as input, and match faces from a collection\n   * or identify celebrities.\n   * @param {IdentifyEntityInput} input - object containing the source image and face match options.\n   * @return {Promise<IdentifyEntityOutput>} Promise resolving to identify results.\n   */\n\n\n  AmazonAIIdentifyPredictionsProvider.prototype.identifyEntities = function (input) {\n    return __awaiter(this, void 0, void 0, function () {\n      var credentials, _a, _b, _c, region, _d, celebrityDetectionEnabled, _e, _f, _g, collectionIdConfig, _h, maxFacesConfig, inputImage, param, recognizeCelebritiesCommand, data, faces, err_7, _j, _k, collectionId, _l, maxFaces, updatedParam, searchFacesByImageCommand, data, faces, err_8, detectFacesCommand, data, faces, err_9;\n\n      var _this = this;\n\n      return __generator(this, function (_m) {\n        switch (_m.label) {\n          case 0:\n            return [4\n            /*yield*/\n            , Credentials.get()];\n\n          case 1:\n            credentials = _m.sent();\n            if (!credentials) return [2\n            /*return*/\n            , Promise.reject('No credentials')];\n            _a = this._config.identifyEntities, _b = _a === void 0 ? {} : _a, _c = _b.region, region = _c === void 0 ? '' : _c, _d = _b.celebrityDetectionEnabled, celebrityDetectionEnabled = _d === void 0 ? false : _d, _e = _b.defaults, _f = _e === void 0 ? {} : _e, _g = _f.collectionId, collectionIdConfig = _g === void 0 ? '' : _g, _h = _f.maxEntities, maxFacesConfig = _h === void 0 ? 50 : _h; // default arguments\n\n            this.rekognitionClient = new RekognitionClient({\n              region: region,\n              credentials: credentials,\n              customUserAgent: getAmplifyUserAgent()\n            });\n            return [4\n            /*yield*/\n            , this.configureSource(input.entities.source).then(function (data) {\n              return inputImage = data;\n            }).catch(function (err) {\n              return Promise.reject(err);\n            })];\n\n          case 2:\n            _m.sent();\n\n            param = {\n              Image: inputImage\n            };\n            if (!(isIdentifyCelebrities(input.entities) && input.entities.celebrityDetection)) return [3\n            /*break*/\n            , 7];\n\n            if (!celebrityDetectionEnabled) {\n              return [2\n              /*return*/\n              , Promise.reject('Error: You have to enable celebrity detection first')];\n            }\n\n            _m.label = 3;\n\n          case 3:\n            _m.trys.push([3, 5,, 6]);\n\n            recognizeCelebritiesCommand = new RecognizeCelebritiesCommand(param);\n            return [4\n            /*yield*/\n            , this.rekognitionClient.send(recognizeCelebritiesCommand)];\n\n          case 4:\n            data = _m.sent();\n            faces = data.CelebrityFaces.map(function (celebrity) {\n              return {\n                boundingBox: makeCamelCase(celebrity.Face.BoundingBox),\n                landmarks: makeCamelCaseArray(celebrity.Face.Landmarks),\n                metadata: __assign(__assign({}, makeCamelCase(celebrity, ['Id', 'Name', 'Urls'])), {\n                  pose: makeCamelCase(celebrity.Face.Pose)\n                })\n              };\n            });\n            return [2\n            /*return*/\n            , {\n              entities: faces\n            }];\n\n          case 5:\n            err_7 = _m.sent();\n            return [2\n            /*return*/\n            , Promise.reject(err_7)];\n\n          case 6:\n            return [3\n            /*break*/\n            , 15];\n\n          case 7:\n            if (!(isIdentifyFromCollection(input.entities) && input.entities.collection)) return [3\n            /*break*/\n            , 12];\n            _j = input.entities, _k = _j.collectionId, collectionId = _k === void 0 ? collectionIdConfig : _k, _l = _j.maxEntities, maxFaces = _l === void 0 ? maxFacesConfig : _l;\n            updatedParam = __assign(__assign({}, param), {\n              CollectionId: collectionId,\n              MaxFaces: maxFaces\n            });\n            _m.label = 8;\n\n          case 8:\n            _m.trys.push([8, 10,, 11]);\n\n            searchFacesByImageCommand = new SearchFacesByImageCommand(updatedParam);\n            return [4\n            /*yield*/\n            , this.rekognitionClient.send(searchFacesByImageCommand)];\n\n          case 9:\n            data = _m.sent();\n            faces = data.FaceMatches.map(function (val) {\n              return {\n                boundingBox: makeCamelCase(val.Face.BoundingBox),\n                metadata: {\n                  externalImageId: _this.decodeExternalImageId(val.Face.ExternalImageId),\n                  similarity: val.Similarity\n                }\n              };\n            });\n            return [2\n            /*return*/\n            , {\n              entities: faces\n            }];\n\n          case 10:\n            err_8 = _m.sent();\n            return [2\n            /*return*/\n            , Promise.reject(err_8)];\n\n          case 11:\n            return [3\n            /*break*/\n            , 15];\n\n          case 12:\n            _m.trys.push([12, 14,, 15]);\n\n            detectFacesCommand = new DetectFacesCommand(param);\n            return [4\n            /*yield*/\n            , this.rekognitionClient.send(detectFacesCommand)];\n\n          case 13:\n            data = _m.sent();\n            faces = data.FaceDetails.map(function (detail) {\n              // face attributes keys we want to extract from Rekognition's response\n              var attributeKeys = ['Smile', 'Eyeglasses', 'Sunglasses', 'Gender', 'Beard', 'Mustache', 'EyesOpen', 'MouthOpen'];\n              var faceAttributes = makeCamelCase(detail, attributeKeys);\n\n              if (detail.Emotions) {\n                faceAttributes['emotions'] = detail.Emotions.map(function (emotion) {\n                  return emotion.Type;\n                });\n              }\n\n              return {\n                boundingBox: makeCamelCase(detail.BoundingBox),\n                landmarks: makeCamelCaseArray(detail.Landmarks),\n                ageRange: makeCamelCase(detail.AgeRange),\n                attributes: makeCamelCase(detail, attributeKeys),\n                metadata: {\n                  confidence: detail.Confidence,\n                  pose: makeCamelCase(detail.Pose)\n                }\n              };\n            });\n            return [2\n            /*return*/\n            , {\n              entities: faces\n            }];\n\n          case 14:\n            err_9 = _m.sent();\n            return [2\n            /*return*/\n            , Promise.reject(err_9)];\n\n          case 15:\n            return [2\n            /*return*/\n            ];\n        }\n      });\n    });\n  };\n\n  AmazonAIIdentifyPredictionsProvider.prototype.decodeExternalImageId = function (externalImageId) {\n    return ('' + externalImageId).replace(/::/g, '/');\n  };\n\n  return AmazonAIIdentifyPredictionsProvider;\n}(AbstractIdentifyPredictionsProvider);\n\nexport { AmazonAIIdentifyPredictionsProvider };\n/**\n * @deprecated use named import\n */\n\nexport default AmazonAIIdentifyPredictionsProvider;","map":null,"metadata":{},"sourceType":"module"}